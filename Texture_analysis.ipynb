{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e35d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from textstat.textstat import textstatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2721ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_file = pd.read_excel(\"Input.xlsx\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a0c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xl_file.index:\n",
    "    url = xl_file[\"URL\"][x]\n",
    "    urlid = xl_file[\"URL_ID\"][x]\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n",
    "        }\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = bs(r.content, 'html5lib')\n",
    "    # print(soup.prettify())\n",
    "    mydivs = soup.find_all(\"h1\", {\"class\": \"entry-title\"})\n",
    "    mydivs += soup.find_all(\"div\", {\"class\": \"td-post-content\"})\n",
    "    \n",
    "    for mydiv in mydivs:\n",
    "        text = mydiv.text\n",
    "    \n",
    "    f = open(\"{}.txt\".format(urlid), \"x\",encoding=\"utf-8\")\n",
    "    f.write(text)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfc5428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pos = open(\"positive-words.txt\", \"r\")\n",
    "neg = open(\"negative-words.txt\", \"r\")\n",
    "positive_words = pos.read()\n",
    "negative_words = neg.read()\n",
    "\n",
    "columns_name = [\"URL_ID\", \"URL\",\"POSITIVE SCORE\",\"NEAGTIVE SCORE\",\"POLARITY SCORE\",\"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"]\n",
    "final_df = pd.DataFrame(columns = columns_name)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6746dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xl_file.index:\n",
    "    url = xl_file[\"URL\"][x]\n",
    "    urlid = xl_file[\"URL_ID\"][x]\n",
    "    \n",
    "    f = open('{}.txt'.format(urlid), \"r\", encoding=\"utf8\")\n",
    "    content = f.read()\n",
    "    \n",
    "    # lemmatization and cleanning\n",
    "    sentence = sent_tokenize(content)\n",
    "    corp = str(content).lower() \n",
    "    corp = re.sub('[^a-zA-Z]+',' ', corp).strip() \n",
    "    tokens = word_tokenize(corp)\n",
    "    words = [t for t in tokens if t not in set(stopwords.words(\"english\"))]\n",
    "    lemmatize = [lemmatizer.lemmatize(w) for w in words]\n",
    "    total_clean_words = len(lemmatize)\n",
    "    \n",
    "    positive_score = 0\n",
    "    negative_score = 0\n",
    "    \n",
    "    for i in lemmatize:\n",
    "        if i in positive_words :\n",
    "            positive_score += 1\n",
    "        if i in negative_words :\n",
    "            negative_score += -1\n",
    "\n",
    "    # positive and negative score\n",
    "    positive_score\n",
    "    negative_score = negative_score * -1\n",
    "    \n",
    "    # polarity score\n",
    "    polarity_score =  (positive_score - negative_score)/((positive_score + negative_score) + 0.000001)\n",
    "    \n",
    "    #subjective score\n",
    "    subjective_score = (positive_score + negative_score)/ ((total_clean_words) + 0.000001)\n",
    "    \n",
    "    #average sentence length\n",
    "    average_sentence_length = len(words)/len(sentence) \n",
    "    \n",
    "    #average word per sentence\n",
    "    average_word_per_sentence = len(words)/len(sentence) \n",
    "    \n",
    "    #complex word count\n",
    "    complex_words = set()\n",
    "    for word in lemmatize:\n",
    "        syllable_count = textstatistics().syllable_count(word)\n",
    "        if syllable_count >= 2:\n",
    "            complex_words.add(word)\n",
    "    complex_word_count = len(complex_words)\n",
    "    \n",
    "    #percent of complex word\n",
    "    percent_complex_words = len(complex_words) / len(lemmatize)\n",
    "    \n",
    "    #fog index\n",
    "    fog_index = 0.4 * (average_sentence_length + percent_complex_words)\n",
    "    \n",
    "    #average syllable per word\n",
    "    syllable_count = textstatistics().syllable_count(word)\n",
    "    ASPW = syllable_count / len(lemmatize)\n",
    "    \n",
    "    # personal pronouns\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    pronouns = pronounRegex.findall(content)\n",
    "    personal_pronouns = len(pronouns)\n",
    "    \n",
    "    #average word length\n",
    "    average_word_length = sum(len(word) for word in lemmatize) / len(lemmatize)\n",
    "    \n",
    "    new_row = {\"URL_ID\":urlid, \"URL\":url,\"POSITIVE SCORE\":positive_score,\"NEAGTIVE SCORE\":negative_score,\"POLARITY SCORE\":polarity_score,\n",
    "               \"SUBJECTIVITY SCORE\":subjective_score,\"AVG SENTENCE LENGTH\":average_sentence_length,\"PERCENTAGE OF COMPLEX WORDS\":percent_complex_words,\n",
    "               \"FOG INDEX\":fog_index,\"AVG NUMBER OF WORDS PER SENTENCE\":average_word_per_sentence,\"COMPLEX WORD COUNT\":complex_word_count,\n",
    "               \"WORD COUNT\":total_clean_words,\"SYLLABLE PER WORD\":ASPW,\"PERSONAL PRONOUNS\":personal_pronouns,\"AVG WORD LENGTH\":average_word_length\n",
    "              }\n",
    "    final_df = final_df.append(new_row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2a63e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.set_index('URL_ID', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0484697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the name of the file\n",
    "file_name = 'final_output_01.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "final_df.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f05965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
